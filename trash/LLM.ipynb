{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from E:\\lm-studio\\models\\uonlp\\Vistral-7B-Chat-gguf\\ggml-vistral-7B-chat-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,38369]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,38369]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,38369]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 7\n",
      "llm_load_vocab: token to piece cache size = 0.1979 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 38369\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 7.29 B\n",
      "llm_load_print_meta: model size       = 3.86 GiB (4.55 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce GTX 960M, compute capability 5.0, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 5 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 5/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3952.27 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =   585.16 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   216.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    40.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.15 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   213.89 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    12.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 301\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': '.', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '2', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load the model\n",
    "llm = Llama(model_path = \"models/smollm-135m.gguf\", verbose=False, n_gpu_layers=20, n_ctx=2048)\n",
    "# llm = Llama(model_path = r\"E:\\lm-studio\\models\\uonlp\\Vistral-7B-Chat-gguf\\ggml-vistral-7B-chat-q4_0.gguf\", verbose=True, device='cuda', n_gpu_layers=5, n_ctx=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA error: the launch timed out and was terminated\n",
      "  current device: 0, in function ggml_cuda_op_mul_mat at D:\\a\\llama-cpp-python\\llama-cpp-python\\vendor\\llama.cpp\\ggml\\src\\ggml-cuda.cu:1613\n",
      "  ggml_cuda_cpy_tensor_2d( src1_ddf_i, src1, i03, i02, src1_col_0, src1_col_0+src1_ncols, stream)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "output = llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"Bạn là một chatbot trả lời câu hỏi dựa trên thông tin tôi cung cấp: Tướng Iran thiệt mạng cùng thủ lĩnh Hezbollah, Nga phản ứng Truyền thông Iran đưa tin một chỉ huy cấp cao của IRGC đã thiệt mạng cùng thủ lĩnh Hezbollah Sayyed Hassan Nasrallah trong cuộc không kích của Israel. Tướng Iran thiệt mạng cùng thủ lĩnh Hezbollah, Nga phản ứng - Ảnh 1. Phó chỉ huy lực lượng Vệ binh cách mạng Hồi giáo Iran (IRGC) Abbas Nilforoushan - Ảnh: IRAN INTERNATIONAL Ngày 28-9, ngay sau khi nhóm Hezbollah tại Lebanon xác nhận thủ lĩnh Sayyed Hassan Nasrallah thiệt mạng trong cuộc không kích của Israel vào Beirut, truyền thông Iran đưa tin tướng Abbas Nilforoushan, phó chỉ huy lực lượng Vệ binh cách mạng Hồi giáo Iran (IRGC), đã chết bên cạnh ông Nasrallah. Trong khi đó, Phó tổng thống thứ nhất của Iran Mohammad Reza Aref tuyên bố việc Tel Aviv giết chết thủ lĩnh Hezbollah Nasrallah trong cuộc không kích hôm 27-9 sẽ dẫn đến sự hủy diệt của Israel. Chúng tôi cảnh báo lãnh đạo của chế độ đang chiếm đóng rằng việc đổ máu bất công, đặc biệt là của Tổng thư ký Hezbollah Seyyed Hassan Nasrallah, sẽ dẫn đến sự hủy diệt của họ, Hãng tin ISNA của Iran trích lời ông Aref. Israel và Hezbollah: Cuộc tỉ thí cuối cùng? Trước đó cùng ngày 28-9, Lãnh đạo tối cao Iran Ayatollah Ali Khamenei đã kêu gọi người Hồi giáo sát cánh bên người dân Lebanon và nhóm Hezbollah bằng bất cứ phương tiện nào trong việc đối đầu với Israel, Hãng tin Reuters đưa tin. Số phận của khu vực này sẽ được quyết định bởi các lực lượng kháng chiến, với nhóm Hezbollah đi đầu, truyền thông nhà nước Iran dẫn lời ông Khamenei. Trong động thái phản ứng, Tổng thống Thổ Nhĩ Kỳ Tayyip Erdogan lên án các cuộc tấn công gần đây của Israel vào Lebanon như một phần trong chính sách mà ông gọi là diệt chủng, chiếm đóng và xâm lược của Israel, đồng thời kêu gọi Hội đồng Bảo an Liên Hiệp Quốc và các cơ quan khác ngăn chặn Tel Aviv. Trong một bài đăng trên X không nhắc đến tên thủ lĩnh Hezbollah, ông Erdogan nói rằng Thổ Nhĩ Kỳ sát cánh cùng người dân và Chính phủ Lebanon; đồng thời gửi lời chia buồn đến những người thiệt mạng trong các cuộc tấn công của Israel, nói rằng thế giới Hồi giáo nên thể hiện lập trường kiên quyết hơn. Cùng lúc đó, Bộ Ngoại giao Nga trong một tuyên bố cũng lên án mạnh mẽ việc Israel giết chết thủ lĩnh Hezbollah Nasralla, gọi đây là một vụ ám sát chính trị khác. Hành động mạnh mẽ này tiềm ẩn những hậu quả nghiêm trọng hơn đối với Lebanon và toàn bộ Trung Đông, Bộ Ngoại giao Nga cho biết. Phía Israel không thể không nhận ra mối nguy này, nhưng đã thực hiện bước đi giết hại công dân Lebanon, điều này gần như chắc chắn sẽ gây ra một đợt bùng phát bạo lực mới. Vì vậy, họ phải chịu hoàn toàn trách nhiệm về sự leo thang sau đó, bộ này nói thêm. Cũng trong ngày 28-9, phát biểu trước Hội đồng Bảo an Liên Hiệp Quốc, Ngoại trưởng Trung Quốc Vương Nghị khẳng định việc đạt được một lệnh ngừng bắn toàn diện ở Trung Đông không nên bị trì hoãn, và giải pháp hai nhà nước giữa Israel và Palestine vẫn là một lối thoát cho các căng thẳng trong khu vực. Hơn 50.000 người tại Lebanon chạy nạn sang Syria Ngày 28-9, người đứng đầu Cao ủy Liên Hiệp Quốc về người tị nạn (UNHCR) Filippo Grandi cho biết hơn 50.000 người tại Lebanon đã chạy sang Syria, trong bối cảnh gia tăng các cuộc không kích của Israel vào các vị trí của phong trào Hồi giáo Hezbollah tại Lebanon. Trên mạng xã hội X, ông Grandi nêu rõ con số trên bao gồm cả người dân Lebanon và Syria. Trong khi đó, giao tranh cũng buộc hơn 200.000 người phải di tản bên trong Lebanon. Theo ông Grandi, các hoạt động cứu trợ đang được tiến hành với sự phối hợp với cả hai chính phủ. Theo UNHCR, tổng số người phải rời bỏ nhà cửa ở Lebanon hiện đã lên tới 211.319 người, trong đó số người di dời kể từ khi Israel tăng cường các cuộc không kích vào ngày 23-9 là 118.000 người. Israel hiện đã chuyển trọng tâm chiến dịch quân sự từ Gaza sang Lebanon. Bộ Y tế Lebanon ước tính giao tranh xuyên biên giới leo thang trong tuần qua đã cướp đi sinh mạng của hơn 700 người. Phần lớn các nạn nhân thiệt mạng vào ngày 23-9, được xem là ngày bạo lực đẫm máu nhất kể từ cuộc nội chiến giai đoạn 1975 - 1990 của Lebanon.\"},\n",
    "          {\"role\": \"user\",\"content\": \"Ai ủng hộ Chính phủ Lebanon\"}\n",
    "      ]\n",
    ")\n",
    "print(output['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  218001.28 ms\n",
      "llama_print_timings:      sample time =      60.80 ms /   150 runs   (    0.41 ms per token,  2467.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =  217993.43 ms /     6 tokens (36332.24 ms per token,     0.03 tokens per second)\n",
      "llama_print_timings:        eval time =  412273.24 ms /   149 runs   ( 2766.93 ms per token,     0.36 tokens per second)\n",
      "llama_print_timings:       total time =  631370.73 ms /   155 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output:ngày xửa ngày xưa, trong một thị trấn nhỏ và đẹp đẽ, có một người phụ nữ tốt bụng tên là Bà Johnson. Bà Johnson được biết đến khắp thị trấn với sự hào phóng, lòng tốt và tình yêu dành cho những người hàng xóm của mình.\n",
      "<<SYS>>, trong một căn phòng nhỏ và đẹp đẽ, có một người phụ nữ trẻ tên là Emily. Emily là một nghệ sĩ tài năng nhưng cuộc sống của cô thật khó khăn, và cô đã phải đấu tranh để kiếm sống. Một ngày nọ, khi đi qua khu phố của mình, Emily đã tình cờ gặp bà Johnson và nhận thấy căn hộ của bà có một căn phòng trống.\n",
      "\n",
      "\"Emily, em có muốn thuê một phòng nhỏ trong nhà tôi không?\" Bà Johnson hỏi\n"
     ]
    }
   ],
   "source": [
    "# Define the generate function\n",
    "def generate(params):\n",
    "    # print(\"Prompt:\", params[\"prompt\"])  # Print the prompt\n",
    "    output = llm(params[\"prompt\"], max_tokens=params[\"max_tokens\"], stop=params[\"stop\"], echo=params[\"echo\"])\n",
    "    # Extract the generated text from the output\n",
    "    generated_text = output['choices'][0]['text']  # Access the text from the first choice\n",
    "    return generated_text  # Return the generated text\n",
    "\n",
    "# Define your parameters\n",
    "params = {\n",
    "    \"prompt\": \"ngày xửa ngày xưa\",\n",
    "    \"max_tokens\": 150,     # Maximum number of tokens to generate\n",
    "    \"stop\": None,         # Define stopping criteria (or set to None)\n",
    "    \"echo\": True          # Whether to include the prompt in the output\n",
    "}\n",
    "\n",
    "print(\"Processing...\")\n",
    "# Generate text\n",
    "output = generate(params)\n",
    "print(f\"Generated Output:{output}\")  # Print only the generated text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ctransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to create LLM 'gguf' from 'E:\\lm-studio\\models\\HuggingFaceTB\\smollm-135M-instruct-v0.2-Q8_0-GGUF\\smollm-135m-instruct-add-basics-q8_0.gguf'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mctransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mE:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mlm-studio\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mHuggingFaceTB\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msmollm-135M-instruct-v0.2-Q8_0-GGUF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msmollm-135m-instruct-add-basics-q8_0.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Define the chat template manually\u001b[39;00m\n\u001b[0;32m     11\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     12\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a friendly chatbot who always responds in the style of a pirate\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     13\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow many helicopters can a human eat in one sitting?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     14\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know!\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     15\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAre you sure?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     16\u001b[0m ]\n",
      "File \u001b[1;32me:\\miniconda3\\envs\\LLM\\lib\\site-packages\\ctransformers\\hub.py:175\u001b[0m, in \u001b[0;36mAutoModelForCausalLM.from_pretrained\u001b[1;34m(cls, model_path_or_repo_id, model_type, model_file, config, lib, local_files_only, revision, hf, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    168\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_find_model_path_from_repo(\n\u001b[0;32m    169\u001b[0m         model_path_or_repo_id,\n\u001b[0;32m    170\u001b[0m         model_file,\n\u001b[0;32m    171\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    172\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    173\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hf:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\n",
      "File \u001b[1;32me:\\miniconda3\\envs\\LLM\\lib\\site-packages\\ctransformers\\llm.py:253\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[1;34m(self, model_path, model_type, config, lib)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib\u001b[38;5;241m.\u001b[39mctransformers_llm_create(\n\u001b[0;32m    248\u001b[0m     model_path\u001b[38;5;241m.\u001b[39mencode(),\n\u001b[0;32m    249\u001b[0m     model_type\u001b[38;5;241m.\u001b[39mencode(),\n\u001b[0;32m    250\u001b[0m     config\u001b[38;5;241m.\u001b[39mto_struct(),\n\u001b[0;32m    251\u001b[0m )\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to create LLM \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m architecture \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctransformers_llm_architecture()\u001b[38;5;241m.\u001b[39mdecode()\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m architecture:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to create LLM 'gguf' from 'E:\\lm-studio\\models\\HuggingFaceTB\\smollm-135M-instruct-v0.2-Q8_0-GGUF\\smollm-135m-instruct-add-basics-q8_0.gguf'."
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    r\"E:\\lm-studio\\models\\uonlp\\Vistral-7B-Chat-gguf\",\n",
    "    model_file=\"ggml-vistral-7B-chat-q4_0.gguf\",\n",
    "    # model_type=\"mistral\",   # Model type is set to 'mistral'\n",
    ")\n",
    "\n",
    "# Define the chat template manually\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I don't know!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Are you sure?\"},\n",
    "]\n",
    "\n",
    "# Concatenate the messages manually to send them as input to the model\n",
    "chat_history = \"\"\n",
    "for message in messages:\n",
    "    chat_history += f\"{message['role']}: {message['content']}\\n\"\n",
    "\n",
    "# Generate a response\n",
    "response = model(chat_history)\n",
    "print(\"Model's response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to create LLM 'gguf' from 'E:\\Download\\Tu_Lieu\\Cao_Hoc\\Ky_3\\Mo_hinh_ngon_ngu_lon\\qa-with-website\\models\\smollm-135m.gguf'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m accelerator \u001b[38;5;241m=\u001b[39m Accelerator()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msmollm-135m.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Prepare the model with the accelerator\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m accelerator\u001b[38;5;241m.\u001b[39mprepare(model)\n",
      "File \u001b[1;32me:\\miniconda3\\envs\\LLM\\lib\\site-packages\\ctransformers\\hub.py:175\u001b[0m, in \u001b[0;36mAutoModelForCausalLM.from_pretrained\u001b[1;34m(cls, model_path_or_repo_id, model_type, model_file, config, lib, local_files_only, revision, hf, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    168\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_find_model_path_from_repo(\n\u001b[0;32m    169\u001b[0m         model_path_or_repo_id,\n\u001b[0;32m    170\u001b[0m         model_file,\n\u001b[0;32m    171\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    172\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    173\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hf:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\n",
      "File \u001b[1;32me:\\miniconda3\\envs\\LLM\\lib\\site-packages\\ctransformers\\llm.py:253\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[1;34m(self, model_path, model_type, config, lib)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib\u001b[38;5;241m.\u001b[39mctransformers_llm_create(\n\u001b[0;32m    248\u001b[0m     model_path\u001b[38;5;241m.\u001b[39mencode(),\n\u001b[0;32m    249\u001b[0m     model_type\u001b[38;5;241m.\u001b[39mencode(),\n\u001b[0;32m    250\u001b[0m     config\u001b[38;5;241m.\u001b[39mto_struct(),\n\u001b[0;32m    251\u001b[0m )\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to create LLM \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m architecture \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctransformers_llm_architecture()\u001b[38;5;241m.\u001b[39mdecode()\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m architecture:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to create LLM 'gguf' from 'E:\\Download\\Tu_Lieu\\Cao_Hoc\\Ky_3\\Mo_hinh_ngon_ngu_lon\\qa-with-website\\models\\smollm-135m.gguf'."
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Initialize the accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    r\"E:\\lm-studio\\models\\uonlp\\Vistral-7B-Chat-gguf\",\n",
    "    model_file=\"ggml-vistral-7B-chat-q4_0.gguf\",\n",
    "    gpu_layers=50,\n",
    ")\n",
    "\n",
    "# Prepare the model with the accelerator\n",
    "model = accelerator.prepare(model)\n",
    "\n",
    "# Check if the model is on GPU\n",
    "print(next(model.parameters()).device)  # Should show 'cuda:0' if on GPU\n",
    "\n",
    "# Define chat history\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I don't know!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Are you sure?\"},\n",
    "]\n",
    "\n",
    "# Concatenate messages to form input\n",
    "chat_history = \"\"\n",
    "for message in messages:\n",
    "    chat_history += f\"{message['role']}: {message['content']}\\n\"\n",
    "\n",
    "# Generate a response\n",
    "response = model(chat_history)\n",
    "print(\"Model's response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
