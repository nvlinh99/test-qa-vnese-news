{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9529727,"sourceType":"datasetVersion","datasetId":5803442}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install environments","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers\n!pip install -q \"pinecone-client[grpc]\"","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-10-03T10:05:02.233093Z","iopub.execute_input":"2024-10-03T10:05:02.233424Z","iopub.status.idle":"2024-10-03T10:05:28.810989Z","shell.execute_reply.started":"2024-10-03T10:05:02.233389Z","shell.execute_reply":"2024-10-03T10:05:28.810010Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.5 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.25.5 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\nkfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from pinecone.grpc import PineconeGRPC as Pinecone\nfrom pinecone import ServerlessSpec\n\npc = Pinecone(api_key='b52dac1e-0eb8-47d3-b5ca-ef64ab2dbfcd')\nindex_name = \"vn-news\"\nindex = pc.Index(index_name)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:05:28.812949Z","iopub.execute_input":"2024-10-03T10:05:28.813304Z","iopub.status.idle":"2024-10-03T10:05:29.285420Z","shell.execute_reply.started":"2024-10-03T10:05:28.813259Z","shell.execute_reply":"2024-10-03T10:05:29.284627Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Running on {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:12:14.613533Z","iopub.execute_input":"2024-10-03T10:12:14.614218Z","iopub.status.idle":"2024-10-03T10:12:14.619747Z","shell.execute_reply.started":"2024-10-03T10:12:14.614170Z","shell.execute_reply":"2024-10-03T10:12:14.618639Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Running on cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **Model Translate**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nmodel_name = \"VietAI/envit5-translation\"\ntokenizer_translate = AutoTokenizer.from_pretrained(model_name)  \nmodel_translate = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:12:16.892296Z","iopub.execute_input":"2024-10-03T10:12:16.893369Z","iopub.status.idle":"2024-10-03T10:12:18.774413Z","shell.execute_reply.started":"2024-10-03T10:12:16.893328Z","shell.execute_reply":"2024-10-03T10:12:18.773340Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **Model embedding**","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel_path = 'Alibaba-NLP/gte-large-en-v1.5'\ntokenizer_embedding = AutoTokenizer.from_pretrained(model_path)\nmodel_embedding = AutoModel.from_pretrained(model_path, trust_remote_code=True).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:12:20.937109Z","iopub.execute_input":"2024-10-03T10:12:20.938122Z","iopub.status.idle":"2024-10-03T10:12:22.032984Z","shell.execute_reply.started":"2024-10-03T10:12:20.938081Z","shell.execute_reply":"2024-10-03T10:12:22.031937Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## **Model LLM**","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()  # same as before\n\n## use this token\n## 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","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:09:33.624017Z","iopub.execute_input":"2024-10-03T10:09:33.624770Z","iopub.status.idle":"2024-10-03T10:09:33.649334Z","shell.execute_reply.started":"2024-10-03T10:09:33.624731Z","shell.execute_reply":"2024-10-03T10:09:33.648385Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22606684d0c64b1b856786285142729b"}},"metadata":{}}]},{"cell_type":"code","source":"# Load model directly\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer_LLM = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\nmodel_LLM = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2-2b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:12:48.470415Z","iopub.execute_input":"2024-10-03T10:12:48.471410Z","iopub.status.idle":"2024-10-03T10:12:51.874319Z","shell.execute_reply.started":"2024-10-03T10:12:48.471370Z","shell.execute_reply":"2024-10-03T10:12:51.873461Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4296fd4fb70143b69d141a2dd3cbbfc1"}},"metadata":{}}]},{"cell_type":"markdown","source":"## **Function core**","metadata":{}},{"cell_type":"markdown","source":"**Function translate**","metadata":{}},{"cell_type":"code","source":"def translate_vi2eng(input_text):\n    input_text = [f\"vi: {input_text}\"]\n    output_encodes = model_translate.generate(tokenizer_translate(input_text, return_tensors=\"pt\", padding=True).input_ids.to(device), max_length=1024)\n    output = tokenizer_translate.batch_decode(output_encodes, skip_special_tokens=True)    \n    return output[0].split(\":\", 1)[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def translate_eng2vi(input_text):\n    input_text = [f\"en: {input_text}\"]\n    output_encodes = model_translate.generate(tokenizer_translate(input_text, return_tensors=\"pt\", padding=True).input_ids.to(device), max_length=1024)\n    output = tokenizer_translate.batch_decode(output_encodes, skip_special_tokens=True)    \n    return output[0].split(\":\", 1)[1]","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:21:40.935297Z","iopub.execute_input":"2024-10-03T10:21:40.936110Z","iopub.status.idle":"2024-10-03T10:21:40.941356Z","shell.execute_reply.started":"2024-10-03T10:21:40.936074Z","shell.execute_reply":"2024-10-03T10:21:40.940350Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"**Function embedding**","metadata":{}},{"cell_type":"code","source":"def embedding_text(input_text):\n    # Tokenize the input texts\n    batch_dict = tokenizer_embedding(input_text, max_length=8192, padding=True, truncation=True, return_tensors='pt').to(device)\n\n    outputs = model_embedding(**batch_dict)\n    embeddings = outputs.last_hidden_state[:, 0]\n    embeddings = F.normalize(embeddings, p=2, dim=1).cpu().detach().numpy()[0].tolist()\n    \n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:16:06.157994Z","iopub.execute_input":"2024-10-03T10:16:06.158646Z","iopub.status.idle":"2024-10-03T10:16:06.165577Z","shell.execute_reply.started":"2024-10-03T10:16:06.158607Z","shell.execute_reply":"2024-10-03T10:16:06.164495Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"**Function retrieval**","metadata":{}},{"cell_type":"code","source":"def retrieval_context(vector_embedding,topk): \n    query_results = index.query(\n    #namespace=\"example-namespace\",\n    vector=vector_embedding,\n    include_metadata=True, \n    top_k=topk,\n    include_values=False\n    )\n    list_id = []\n    list_url = []\n    for item in query_results['matches']:\n        list_id.append(int(item[\"id\"]))\n        list_url.append(item[\"metadata\"][\"url\"])\n    return list_id,list_url","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:16:32.137410Z","iopub.execute_input":"2024-10-03T10:16:32.138320Z","iopub.status.idle":"2024-10-03T10:16:32.143855Z","shell.execute_reply.started":"2024-10-03T10:16:32.138279Z","shell.execute_reply":"2024-10-03T10:16:32.142901Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# retrieval_context(embedding_text(\"Tôi muốn tìm kiếm một bản tin về kiện nhau do manga\"), topk = 5)\nretrieval_context(embedding_text(\"tin tức về Tottenham\"), topk = 5)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:16:54.712504Z","iopub.execute_input":"2024-10-03T10:16:54.713268Z","iopub.status.idle":"2024-10-03T10:16:54.843680Z","shell.execute_reply.started":"2024-10-03T10:16:54.713226Z","shell.execute_reply":"2024-10-03T10:16:54.842717Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"([130, 194, 114, 366, 300],\n ['https://vnexpress.net/nuot-hai-goi-heroine-khi-bi-bat-qua-tang-mua-ban-4797709.html\\n',\n  'https://vnexpress.net/con-trai-kien-me-vi-vut-bo-suu-tap-truyen-tranh-manga-4797697.html\\n',\n  'https://vnexpress.net/co-nen-bo-nganh-dieu-duong-de-thi-lai-rang-ham-mat-4796055.html\\n',\n  'https://vnexpress.net/lam-thanh-my-dien-vien-gen-z-duoc-san-don-4795187.html\\n',\n  'https://vnexpress.net/ong-trump-doa-truy-to-google-neu-dac-cu-4797915.html\\n'])"},"metadata":{}}]},{"cell_type":"markdown","source":"**Function mapping data**","metadata":{}},{"cell_type":"code","source":"import pickle\ndef mapping_data(list_id,list_url):\n    \n    with open('/kaggle/input/llm-chatbot/total_output_clean.pkl', 'rb') as file:\n        total_output_clean = pickle.load(file)\n        \n    total_text_with_link = []\n    for index,url in zip(list_id,list_url): \n        total_text_with_link.append(f\"{total_output_clean[index]}, link:{url}\")\n    \n#     with open('/kaggle/input/llm-chatbot/total_chunks.pkl', 'rb') as file:\n#         total_chunks = pickle.load(file)\n    # Turn list to string\n    sentence_list = total_text_with_link\n\n    # Convert the list to a string in the desired format\n    formatted_string = '; '.join([f'\"{sentence}\"' for sentence in sentence_list])\n\n    # Add brackets around the final string\n    result_context = f\"[{formatted_string}]\"\n    \n#     print(result_context)\n    return result_context","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:18:21.801467Z","iopub.execute_input":"2024-10-03T10:18:21.802263Z","iopub.status.idle":"2024-10-03T10:18:21.808673Z","shell.execute_reply.started":"2024-10-03T10:18:21.802225Z","shell.execute_reply":"2024-10-03T10:18:21.807722Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"**Function answer question**","metadata":{}},{"cell_type":"code","source":"import re\ndef chatbot(question,context):\n    from datetime import date\n\n    # Get the current date\n    current_date = date.today()\n#     print(f\"Date: {current_date}\")  # Output: YYYY-MM-DD (e.g., 2024-08-02)\n\n    # Define the chat template using Role 1 (Prompting Specialist)\n    messages = [\n        {\"role\": \"user\", \"content\": f\"You are an expert in understanding user queries and rephrasing them. The original question is: {question}. Rephrase it clearly and concisely in 2 sentences for a QA chatbot to answer. Only return the rephrased question, no extra content or answers.\"},\n    ]\n\n    input_ids_1 = tokenizer_LLM.apply_chat_template(conversation=messages, return_tensors=\"pt\", return_dict=True).to(\"cuda\")\n\n    outputs_1 = model_LLM.generate(**input_ids_1, max_new_tokens=256)\n    decoded_output_1 = tokenizer_LLM.decode(outputs_1[0], skip_special_tokens=False)\n    answer_query_1 = decoded_output_1.rsplit(\"<end_of_turn>\", 2)[1].strip().strip('*') # Because the output include the answer between 2 \"<end_of_turn>\"\n\n#     print(f\"Rephrase question: {answer_query_1}\")\n\n    ###############################################################\n\n    # Define the chat template using Role 2 (QA Chatbot)\n    messages = [\n        {\"role\": \"user\", \"content\": f\"The current date is {current_date} (YYYY-MM-DD format). You are a friendly AI chatbot that looks through the news article and provide answer for user. Answer the question in a natural and friendly tone under 200 words. Have to use Chain of Thought reasoning with no more than three steps but dont include it in the response to user. Here are the new article {context}, the user asks {answer_query_1}. YOU MUST INCLUDE THE LINK TO THE ARTICLE AT THE END OF YOUR ANSWER\"},\n    ]\n\n    input_ids_2 = tokenizer_LLM.apply_chat_template(conversation=messages, return_tensors=\"pt\", return_dict=True).to(\"cuda\")\n\n    outputs_2 = model_LLM.generate(**input_ids_2, max_new_tokens=1024)\n    decoded_output_2 = tokenizer_LLM.decode(outputs_2[0], skip_special_tokens=False)\n    answer_query_2 = decoded_output_2.rsplit(\"<end_of_turn>\", 2)[1].strip().strip('*') # Because the output include the answer between 2 \"<end_of_turn>\"\n    \n    # Regular expression pattern to extract URLs\n    url_pattern = r'https?://[^\\s]+'\n\n    # Find the URL in the text\n    answer_without_url = re.sub(url_pattern, '', answer_query_2)\n    urls = re.findall(url_pattern, answer_query_2)\n\n#     print(f\"Answer: {answer_query_2}\")\n    return answer_without_url,urls[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:18:47.923274Z","iopub.execute_input":"2024-10-03T10:18:47.924008Z","iopub.status.idle":"2024-10-03T10:18:47.934029Z","shell.execute_reply.started":"2024-10-03T10:18:47.923970Z","shell.execute_reply":"2024-10-03T10:18:47.933076Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## **Full Pipeline**","metadata":{}},{"cell_type":"code","source":"def pipeline(question):\n    question_translate = translate_vi2eng(question)\n    question_embedding = embedding_text(question_translate)\n    list_id,list_url = retrieval_context(question_embedding,3)\n    context = mapping_data(list_id,list_url)\n    result,url = chatbot(question_translate,context)\n    answer = translate_eng2vi(result)\n#     print(question_translate)\n#     print(\"----\")\n#     print(question_embedding)\n#     print(\"----\")\n#     print(list_id,list_url)\n#     print(\"----\")\n#     print(context)\n#     print(\"----\")\n#     print(result)\n#     print(\"----\")\n#     print(answer)\n#     print(\"----\")    \n\n    return answer,url   \n    ","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:19:02.529822Z","iopub.execute_input":"2024-10-03T10:19:02.530214Z","iopub.status.idle":"2024-10-03T10:19:02.536143Z","shell.execute_reply.started":"2024-10-03T10:19:02.530165Z","shell.execute_reply":"2024-10-03T10:19:02.535220Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"answer,url = pipeline(\"Tôi cần tin tức về đá bóng\")\nprint(f\"answer:{answer}\")\nprint(f\"url:{url}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:19:25.503164Z","iopub.execute_input":"2024-10-03T10:19:25.504105Z","iopub.status.idle":"2024-10-03T10:19:25.560664Z","shell.execute_reply.started":"2024-10-03T10:19:25.504065Z","shell.execute_reply":"2024-10-03T10:19:25.559535Z"},"trusted":true},"execution_count":34,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m answer,url \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTôi cần tin tức về đá bóng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[33], line 2\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipeline\u001b[39m(question):\n\u001b[0;32m----> 2\u001b[0m     question_translate \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_vi2eng\u001b[49m(question)\n\u001b[1;32m      3\u001b[0m     question_embedding \u001b[38;5;241m=\u001b[39m embedding_text(question_translate)\n\u001b[1;32m      4\u001b[0m     list_id,list_url \u001b[38;5;241m=\u001b[39m retrieval_context(question_embedding,\u001b[38;5;241m3\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'translate_vi2eng' is not defined"],"ename":"NameError","evalue":"name 'translate_vi2eng' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}